---
title: "Text Mining With R"
author: "Abhishek Dangol"
date: "1/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# TIDY TEXT FORMAT

```{r warning = FALSE, echo = TRUE, message = FALSE}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The carriage held but just Ourselves -",
          "and Immortality")
text
```

In order to turn it into a tidy text dataset, we first need to put it into a data frame.

```{r warning = FALSE, echo = TRUE, message = FALSE}
library(dplyr)
text_df <- tibble(line = 1:4, text = text)
text_df
```

Within our tidy text framework, we need to both break the text into individual tokens by a process called tokenization and transform it into a tidy data structure. To do this, we can use tidytext's `unnest_tokens()`

```{r warning = FALSE, echo = TRUE, message = FALSE}
library(tidytext)

text_df %>%
  unnest_tokens(word, text)
```

## Tidying the works of Jane Austen

Use `janeaustenr` package where a line is analogous to a literal printed line in a physical book. Use `mutate()` to annotate a `linenumber` quantity to keep track of lines in the original format and a chapter (using a regex) to find where all the chapters are:

```{r warning = FALSE, echo = TRUE, message = FALSE}
library(janeaustenr)
library(dplyr)
library(stringr)

original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()
(original_books)
```

To work with this as a tidy dataset, we need to restructure it in the one-token-per-row format, which as we saw earlier is done with `unnest_tokens()` function:

```{r warning = FALSE, echo = TRUE, message = FALSE}
library(tidytext)
tidy_books <- original_books %>%
  unnest_tokens(word, text)
tidy_books
```

Often in text analysis, we will want to remove stop words; stop words are words that are not useful for analysis, typically extremely common words such as "the", "of", "to" and so forth in english. We can remove stop words kept in the tidytext dataset as `stop_words` and `anti_join`:

```{r warning = FALSE, echo = TRUE, message = FALSE}
data(stop_words)
tidy_books <- tidy_books %>%
  anti_join(stop_words)
```

We can use dplyr's `count()` to find the most common words in all the books as a whole:

```{r warning = FALSE, echo = TRUE, message = FALSE}
tidy_books %>%
  count(word, sort = TRUE)
```

To create a visualization:

```{r warning = FALSE, echo = TRUE, message = FALSE}
library(ggplot2)
tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + 
  geom_col() + 
  xlab(NULL) +
  coord_flip()
```

## The gutenbergr package

### Word frequencies

Let us get the "The Time Machine", "The War of the Worlds", "The invisible man" and "The Island of Doctor Moreau". We can access these works using `gutenberg_download()` and the Project Gutenberg ID numbers for each novel.

```{r warning = FALSE, cache = TRUE, message = FALSE}
library(gutenbergr)
hgwells <- gutenberg_download(c(35, 36, 5230, 159))
```


```{r warning = FALSE, cache = TRUE, message = FALSE}
tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
```

Let us see what are the most common words in these four novels by HG Wells:

```{r warning = FALSE, cache = TRUE, message = FALSE}
tidy_hgwells %>%
  count(word, sort = TRUE)
```

Let us get "Jane Eyre", "Wuthering Heights", "The Tenant of Wildfell Hall", "Villette", and "Agnes Grey". We will again use the Project Gutenberg ID numbers for each novel and access the texts using `gutenberg_download()`:

```{r warning = FALSE, cache = TRUE, message = FALSE}
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
tidy_bronte <- bronte %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
```

What are the most common words in these novels of the Bronte sisters?

```{r warning = FALSE, cache = TRUE, message = FALSE}
tidy_bronte %>%
  count(word, sort = TRUE)
```

Now let us calculate the frequency for each word for the works of Jane Austen, HG Wells and the Bronte sisters. We can use spread and gather from `tidyr` to reshape the dataframe so that it is just what we need for plotting and comparing the three sets of novels.

```{r warning = FALSE, cache = TRUE, message = FALSE}
library(tidyr)

frequency <- bind_rows(mutate(tidy_bronte, author = "Bronte Sisters"),
                       mutate(tidy_hgwells, author = "H. G. Wells"),
                       mutate(tidy_books, author = "Jane Austen")) %>%
  mutate(word = str_extract(word, "[a-z]+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>%
  select(-n) %>%
  spread(author, proportion) %>%
  gather(author, proportion, `Bronte Sisters`:`H. G. Wells`)

```

We use `str_extract()` because some words have underscores around them to indiacate emphasis. We do not want to count "_any_" separately from "any":
library(scales)

```{r warning = FALSE, cache = TRUE, message = FALSE}
library(scales)
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)

```


Words that are close to the line in these plots have similar frequencies in both sets of texts, for example, in both Austen and Brontë texts (“miss”, “time”, “day” at the upper frequency end) or in both Austen and Wells texts (“time”, “day”, “brother” at the high frequency end). Words that are far from the line are words that are found more in one set of texts than another. For example, in the Austen-Brontë panel, words like “elizabeth”, “emma”, and “fanny” (all proper nouns) are found in Austen’s texts but not much in the Brontë texts, while words like “arthur” and “dog” are found in the Brontë texts but not the Austen texts. In comparing H.G. Wells with Jane Austen, Wells uses words like “beast”, “guns”, “feet”, and “black” that Austen does not, while Austen uses words like “family”, “friend”, “letter”, and “dear” that Wells does not.
Let us quantify how similar and different these sets of word frequencies are using a correlation test. How correlated are the word frequencies between Austen and the Bronte sisters and Wells?

```{r warning = FALSE, cache = TRUE, message = FALSE}
cor.test(data = frequency[frequency$author == "Bronte Sisters",],
         ~ proportion + `Jane Austen`)
```

```{r warning = FALSE, cache = TRUE, message = FALSE}
cor.test(data = frequency[frequency$author == "H. G. Wells",],
         ~ proportion + `Jane Austen`)
```

Word frequencies are more correlated between the Austen and Bronte Novels than between Austen and H. G. Wells.


# SENTIMENT ANALYSIS WITH TIDY DATA

All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words in a binary fashion into positive and negative categories. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. All of this information is tabulated in the sentiments dataset, and tidytext provides a function get_sentiments() to get specific sentiment lexicons without the columns that are not used in that lexicon.

```{r warning = FALSE, cache = TRUE, message = FALSE}
library(tidytext)
get_sentiments("bing")

```

## Sentiment analysis with inner join

We can examine how sentiment changes throughout each novel. We will use 80 lines:
```{r warning = FALSE, cache = TRUE, message = FALSE}
library(tidyr)

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

```{r warning = FALSE, cache = TRUE, message = FALSE}
library(ggplot2)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

## Most common positive and negative words

```{r warning = FALSE, cache = TRUE, message = FALSE}
bing_word_counts <- tidy_books %>% 
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts
```

```{r warning = FALSE, cache = TRUE, message = FALSE}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

## Wordclouds
```{r warning = FALSE, cache = TRUE, message = FALSE}
library(wordcloud)

tidy_books %>% 
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

Most common negative and positive words as wordclouds:
```{r warning = FALSE, cache = TRUE, message = FALSE}
library(reshape2)

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

## Analyzing sentences
```{r warning = FALSE, cache = TRUE, message = FALSE}
PanP_sentences <- tibble(text = prideprejudice) %>%
  unnest_tokens(sentence, text, token = "sentences")
PanP_sentences$sentence[2]
```

## Separating into chapters
```{r warning = FALSE, cache = TRUE, message = FALSE}
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())
```

## For each book, which chapter has the highest proportion of negative words:
```{r warning = FALSE, cache = TRUE, message = FALSE}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  top_n(1) %>%
  ungroup()
```

# ANALYIZING WORD AND DOCUMENT FREQUENCY

## Term frequency in Jane Austen's novels:
```{r warning = FALSE, cache = TRUE, message = FALSE}
library(dplyr)
library(janeaustenr)
library(tidytext)

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE)

total_words <- book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words
```

## Zipf's law states that the frequency that a word appears is inversely proportional to its rank.
```{r warning = FALSE, cache = TRUE, message = FALSE}
freq_by_rank <- book_words %>% 
  group_by(book) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank
```

## The `bind_tf_idf` function
The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents, in this case, the group of Jane Austen’s novels as a whole. Calculating tf-idf attempts to find the words that are important (i.e., common) in a text, but not too common.
```{r warning = FALSE, cache = TRUE, message = FALSE}
book_words <- book_words %>%
  bind_tf_idf(word, book, n)
book_words
```

Let us look at terms with high tf-tdf in Jane Austen's novels:
```{r warning = FALSE, cache = TRUE, message = FALSE}
book_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

Visualization of these high tf-idf words:
```{r warning = FALSE, cache = TRUE, message = FALSE}
book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(book) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  coord_flip()
```

## A corpus of Physics texts

```{r warning = FALSE, cache = TRUE, message = FALSE}
library(gutenbergr)
physics <- gutenberg_download(c(37729, 14725, 13476, 30144),
                              meta_fields = "author")
```

Let us use `unnest_tokens()` and `count()` to find out how many times each word was used in each text.

```{r warning = FALSE, cache = TRUE, message = FALSE}
physics_words <- physics %>%
  unnest_tokens(word, text) %>%
  count(author, word, sort = TRUE)
physics_words
```

Calculate and visualize high tf-idf words:

```{r warning = FALSE, cache = TRUE, message = FALSE}
library(forcats)

plot_physics <- physics_words %>%
  bind_tf_idf(word, author, n) %>%
  mutate(word = fct_reorder(word, tf_idf)) %>%
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christiaan", 
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))

plot_physics %>% 
  group_by(author) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~author, ncol = 2, scales = "free") +
  coord_flip()
```

# RELATIONSHIP BETWEEN WORDS: N-GRAMS AND CORRELATIONS
Many interesting text analyses are based on the relationships between words, whether examining which words tend to follow others immediately, or that tend to occur within the same document. `token = "ngrams"` argument tokenizes by pairs of adjacent words rather than by individual ones. 

## Tokenizing by n-gram
We do this by adding the token = "ngrams" option to unnest_tokens(), and setting n to the number of words we wish to capture in each n-gram. When we set n to 2, we are examining pairs of two consecutive words, often called “bigrams”:
```{r warning = FALSE, cache = TRUE, message = FALSE}
library(dplyr)
library(tidytext)
library(janeaustenr)

austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

austen_bigrams
```

## Counting and filtering n-grams

```{r warning = FALSE, cache = TRUE, message = FALSE}
austen_bigrams %>% 
  count(bigram, sort = TRUE)
```

The most common bigrams are pairs of common (uninteresting words). We can use tidyr's `separate()` which splits a column into multiple based on a delimeter. This lets us separate into two columns "word1" and "word2" at which point we can remove cases where either is a stop-word:
```{r warning = FALSE, cache = TRUE, message = FALSE}
library(tidyr)

bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```

Most common bigrams not containing the stop-words:
```{r warning = FALSE, cache = TRUE, message = FALSE}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```

For most common trigrams, we can use `n=3`:
```{r warning = FALSE, cache = TRUE, message = FALSE}
austen_books() %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)
```

## Analyzing bigrams

To look at the most common "streets" mentioned in each book:
```{r warning = FALSE, cache = TRUE, message = FALSE}
bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)
```

tf-idf of bigrams across Austen novels:
```{r warning = FALSE, cache = TRUE, message = FALSE}
bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```

## Using bigrams to provide context in sentiment analysis
The words "happy" and "like" will be counted as positive, even in a sentence like "I'm not happy and I don't like it". Since we have the date organized into bigrams, it is easy to tell how often words are preceded by a word like "not".

```{r warning = FALSE, cache = TRUE, message = FALSE}
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)
```

## Visualizing a network of bigrams with ggraph
```{r warning = FALSE, cache = TRUE, message = FALSE}
library(igraph)
bigram_counts

# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()
bigram_graph
```

```{r warning = FALSE, cache = TRUE, message = FALSE}
library(ggraph)
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

```{r warning = FALSE, cache = TRUE, message = FALSE}
set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

# CASE STUDY: COMPARING TWITTER ARCHIVES